## Decoder-Only Transformer (from Scratch)

# Overview

This project is a decoder-only reimplementation of the Transformer architecture built from scratch in PyTorch.

It follows Andrej Karpathy’s NanoGPT / “Let’s build GPT” lecture series step by step, with the goal of understanding how GPT-style models work at a low level. This is a learning-focused implementation, not a production-ready system.
 
# Features (So Far)

Token embeddings

Positional embeddings

Multi-head self-attention

Feed-forward networks

Layer normalization

Autoregressive text generation

Basic training loop

This implementation currently matches the early/core parts of Karpathy’s walkthrough. More advanced features shown later have not been added yet.

# Motivation

The goal of this project is to:

Learn how Transformer models are built from first principles

Understand GPT-style architectures without heavy abstractions

Practice implementing deep learning models directly in PyTorch

This repository exists primarily as a personal learning exercise.
